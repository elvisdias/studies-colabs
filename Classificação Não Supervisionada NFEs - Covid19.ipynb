{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Classificação não supervisionada - Covid19.ipynb","provenance":[],"collapsed_sections":["Ugqe6_XDXB8c","e6Psm9Rx0EAY","U34_YKHp9BEL","AvzYNNccz6nc","DR61IzGoo6sJ","YK8l8S79y32r","rSjnIytYpUBg","c0SYzfZEz1hZ","-Gtd4VViWs81","qRwxDQHfU7xK","2hPTM3QohYow","0tQwJ3I8xbxa"],"mount_file_id":"1nSR4dN-t5sq4HApQnSwh_ZPxJODUa3jl","authorship_tag":"ABX9TyNa6wLc/gl2UM+ekPOtbhuI"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"Ugqe6_XDXB8c","colab_type":"text"},"source":["###Imports"]},{"cell_type":"code","metadata":{"id":"46-cDxEMnvAv","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":445},"executionInfo":{"status":"ok","timestamp":1594406300326,"user_tz":180,"elapsed":71214,"user":{"displayName":"Elvis Ferreira","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GifuqBgF4V2Ne4qNmTkno-pwowhPfHvmKIsH7GfKJ0=s64","userId":"11118255313765743639"}},"outputId":"b82b8f1f-1a39-4c91-fbe8-8b67c3705b39"},"source":["!pip install Unidecode\n","!pip install hdbscan\n","import pickle\n","import unidecode\n","import hdbscan\n","import re\n","import nltk\n","from nltk import FreqDist\n","from nltk.util import ngrams\n","from nltk.tokenize import word_tokenize\n","import string\n","from string import punctuation\n","import pandas as pd\n","import numpy as np\n","import collections\n","from tqdm import tqdm\n","from gensim.models import Word2Vec, FastText\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.metrics.pairwise import cosine_similarity, cosine_distances\n","import umap"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Collecting Unidecode\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d0/42/d9edfed04228bacea2d824904cae367ee9efd05e6cce7ceaaedd0b0ad964/Unidecode-1.1.1-py2.py3-none-any.whl (238kB)\n","\r\u001b[K     |█▍                              | 10kB 15.0MB/s eta 0:00:01\r\u001b[K     |██▊                             | 20kB 1.7MB/s eta 0:00:01\r\u001b[K     |████▏                           | 30kB 2.2MB/s eta 0:00:01\r\u001b[K     |█████▌                          | 40kB 2.5MB/s eta 0:00:01\r\u001b[K     |██████▉                         | 51kB 2.0MB/s eta 0:00:01\r\u001b[K     |████████▎                       | 61kB 2.3MB/s eta 0:00:01\r\u001b[K     |█████████▋                      | 71kB 2.5MB/s eta 0:00:01\r\u001b[K     |███████████                     | 81kB 2.7MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 92kB 2.9MB/s eta 0:00:01\r\u001b[K     |█████████████▊                  | 102kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████▏                | 112kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████▌               | 122kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████████▉              | 133kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████████▎            | 143kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████████▋           | 153kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 163kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 174kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████▊       | 184kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 194kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▌    | 204kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▉   | 215kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▎ | 225kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▋| 235kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 245kB 2.8MB/s \n","\u001b[?25hInstalling collected packages: Unidecode\n","Successfully installed Unidecode-1.1.1\n","Collecting hdbscan\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/22/2f/2423d844072f007a74214c1adc46260e45f034bb1679ccadfbb8a601f647/hdbscan-0.8.26.tar.gz (4.7MB)\n","\u001b[K     |████████████████████████████████| 4.7MB 2.6MB/s \n","\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from hdbscan) (1.12.0)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from hdbscan) (0.15.1)\n","Requirement already satisfied: scipy>=0.9 in /usr/local/lib/python3.6/dist-packages (from hdbscan) (1.4.1)\n","Requirement already satisfied: numpy>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from hdbscan) (1.18.5)\n","Requirement already satisfied: scikit-learn>=0.17 in /usr/local/lib/python3.6/dist-packages (from hdbscan) (0.22.2.post1)\n","Requirement already satisfied: cython>=0.27 in /usr/local/lib/python3.6/dist-packages (from hdbscan) (0.29.20)\n","Building wheels for collected packages: hdbscan\n","  Building wheel for hdbscan (PEP 517) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for hdbscan: filename=hdbscan-0.8.26-cp36-cp36m-linux_x86_64.whl size=2307198 sha256=9d61fcb266640cc85f040bcd7efb42d41642629e59f34283265315b028610efa\n","  Stored in directory: /root/.cache/pip/wheels/82/38/41/372f034d8abd271ef7787a681e0a47fc05d472683a7eb088ed\n","Successfully built hdbscan\n","Installing collected packages: hdbscan\n","Successfully installed hdbscan-0.8.26\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"e6Psm9Rx0EAY","colab_type":"text"},"source":["###Funções"]},{"cell_type":"code","metadata":{"id":"3i2GPjRX0CuV","colab_type":"code","colab":{}},"source":["def grava(obj, path, filename):\n","\tpkl_file = open(path + filename, 'wb')\n","\tpickle.dump(obj, pkl_file)\n","\tpkl_file.close()\n","\n","def abre(path, filename):\n","\tpkl_file = open(path + filename, 'rb')\n","\tobj = pickle.load(pkl_file)\n","\tpkl_file.close()\n","\treturn obj\n","\n","def get_nome_representacao_do_grupo_v4(df2,qtd_palavras,unidades):\n","\tpercentual_pra_manter_palavra_na_representacao = 0.50\n","\t#pega cada palavra e ve as que mais se repetem nas sentences\n","\t#fica com aquelas que estao em mais do que X% das sentences\n","\tsentences = [sent for sent in df2['xprod']]\n","\tif len(set(sentences)) == 1: #ou seja, todos os itens sao iguais.\n","\t\trepresentacao_grupo = [word for word in set(sentences)][0].split()\n","\telse:\n","\t\tpalavras_series = df2['xprod'].str.split()\n","\t\tpalavras_series = palavras_series.apply(lambda x: x[:qtd_palavras])\n","\t\tcontagem_palavras_nas_sentences = {}\n","\t\tpalav = [item for sublist in palavras_series for item in sublist]\n","\t\tpalavras = sorted( set(palav), key=palav.index) #pra preservar a ordem em que as palavras aparecem (set puro coloca em ordem alfabetica)\n","\t\tfor palavra in palavras:\n","\t\t\tcontagem_palavras_nas_sentences[palavra] = (palavras_series.apply(lambda x: palavra in x)).sum()\n","\t\n","\t\tcontagem_palavras_nas_sentences = pd.Series(contagem_palavras_nas_sentences)\n","\t\tcontagem_palavras_nas_sentences = contagem_palavras_nas_sentences / len(df2)\n","\t\tcontagem_palavras_nas_sentences = contagem_palavras_nas_sentences[contagem_palavras_nas_sentences > percentual_pra_manter_palavra_na_representacao]\n","\t\trepresentacao_grupo = list(contagem_palavras_nas_sentences.index)\n","\t#reordenacao:\n","\tprimeiras_palavras = [word for word in representacao_grupo if ((not word.isdigit()) and word not in unidades)]\n","\tmeio_palavras = [word for word in representacao_grupo if word.isdigit()]\n","\tultimas_palavras = [word for word in representacao_grupo if word in unidades]\n","\t\n","  # intercala numeros e unidades:\n","\tif len(meio_palavras) == len(ultimas_palavras):\n","\t\tresult = [None]*(len(meio_palavras)+len(ultimas_palavras))\n","\t\tresult[::2] = meio_palavras\n","\t\tresult[1::2] = ultimas_palavras\n","\t\tmeio_palavras = result\n","\t\tultimas_palavras = []\n","\telse:\n","\t\t# if 'x' in representacao_grupo:\n","\t\tif (('x' in ultimas_palavras) and (len(meio_palavras) > 0)):\n","\t\t\tultimas_palavras = [word for word in ultimas_palavras if word != 'x'] #retira o 'x', vai inserir abaixo:\n","\t\t\tmeio_palavras.insert(1,'x') #insere o 'x' apos o 1o numero\n","\tif len(meio_palavras) == 0:\n","\t\trepresentacao_grupo = primeiras_palavras #daih nao coloca unidades\n","\telse:\n","\t\trepresentacao_grupo = primeiras_palavras + meio_palavras + ultimas_palavras\n","\treturn representacao_grupo\n","\n","def print_exemplos_grupos_v2_aleatorio(df,grupos,grupox,cols,qtd_palavras,unidades):\n","\t#inicio = np.random.randint(len(grupos)-qtd_grupos_mostrar)\n","\t#fim = inicio + qtd_grupos_mostrar\n","\tfor grupo in grupos:\n","\t\tdf_mostrar = df[df[grupox] == grupo]\n","\t\tprint('\\nGrupo:',grupo,'len:',len(df_mostrar))\n","\t\tprint(get_nome_representacao_do_grupo_v4(df2=df[df[grupox]==grupo],qtd_palavras=qtd_palavras,unidades=unidades))\n","\t\tprint(df_mostrar[cols])\n","\n","def calc_preco(df, grupos, grupox):\n","\tcols = ['NCM','xprod', 'vuncom']\n","\tdf_list_grupos = []\n","\n","\tgroups = []\n","\tgroup_names = []\n","\tmedia = []\n","\tmediana = []\n","\tmedia_saneada = []\n","\tprecos_max = []\n","\tprecos_min = []\n","\tdescr_max = []\n","\tdescr_min = []\n","\n","\tfor i, grupo in enumerate(grupos):\n","\t\tdf_grupo = df[df[grupox] == grupo]\n","\t\tdf_list_grupos.append(df_grupo)\t\n","\t\t\n","\t\t #S = E – M sobrepreço estimado - referencial mercado\n","\t\tmedia.append(round(df_grupo['vuncom'].mean(),3))\n","\t\tmediana.append(round(df_grupo['vuncom'].median(),3))\n","\t\t\n","\t\tnova_media, preco_max, preco_min = calc_media_saneada(df_grupo)\n","\t\t\n","\t\tmedia_saneada.append(nova_media)\n","\t\tprecos_max.append(preco_max)\n","\t\tprecos_min.append(preco_min)\n","\t\n","\t\trow_max = df_grupo.loc[df_grupo['vuncom'].idxmax()]\n","\t\trow_min = df_grupo.loc[df_grupo['vuncom'].idxmin()]\n","\t\tdescr_max.append(row_max['xprod'])\n","\t\tdescr_min.append(row_min['xprod'])\n","\t\n","\t\tdescricao = get_nome_representacao_do_grupo_v4(df2=df[df[grupox]==grupo],qtd_palavras=qtd_palavras,unidades=unidades)\n","\t\tdescricao = ' '.join(descricao)\n","\t\tgroup_names.append(descricao)\n","\n","\t\tgroups.append(i+1)\n","\t\n","\tdf_precos = pd.DataFrame(zip(groups, group_names, media, mediana, descr_max, precos_max, descr_min, precos_min, media_saneada),\\\n","\t                         columns=[\"Grupo\",\"Descricao\", \"Media\", \"Mediana\", \"Descr_Max\", \"Max\", \"Descr_Min\", \"Min\", \"Media Saneada\"])\n","\t\n","\treturn df_precos, df_list_grupos\n","\n","def calc_media_saneada(df):\n","\tpreco = df['vuncom'].dropna()\n","\tpreco_max = max(preco)\n","\tpreco_min = min(preco)\n","\tpreco_mean = preco.mean()\n","\tLS = preco_mean + preco.std()\n","\tLI = preco_mean - preco.std()\t\t\n","\tCV = abs(preco.std() / preco_mean)\n","\t\n","\tif CV > 0.25:\n","\t\tprecos = preco[preco.between(LI, LS)]\n","\t\treturn (precos.mean(), preco_max, preco_min)\n","\telse:\n","\t\treturn (preco_mean, preco_max, preco_min)\n","\n","def descricoes_grupos(df, grupos, grupox):\n","\tgroups = []\n","\tdescrs = []\n","\tfor i, grupo in enumerate(grupos):\n","\t\tdf_grupo = df[df[grupox] == grupo]\n","\t\tdescr_unicas = df_grupo['xprod'].unique()\n","\t\tfor descr in descr_unicas:\n","\t\t\tgroups.append(i+1)\n","\t\t\tdescrs.append(descr)\n","\tdf_descrs_unicas = pd.DataFrame(zip(descrs, groups), columns=[\"Descricao\", \"Grupo\"])\n","\treturn df_descrs_unicas\n","\n","def save_groups(dfs):\n","\tfor i, df in enumerate(dfs):\n","\t\tdf.to_excel(f'grupo{i}.xlsx')\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"U34_YKHp9BEL","colab_type":"text"},"source":["###Variaveis"]},{"cell_type":"code","metadata":{"id":"IyivCrRF9DNn","colab_type":"code","colab":{}},"source":["qtd_dimensoes = 300\n","qtd_dimensoes_umap = 10\n","qtd_palavras = 10\n","tamanho_minimo_pra_formar_grupo = 35\n","quantile_a_retirar_outliers_dbscan = 0.90\n","percentual_primeira_palavra_igual_pra_considerar_grupo_homogeneo = 0.70\n","quantile_a_retirar_quantidade_palavras_diferentes_no_grupo = 0.95\n","similarity_minima_pra_juntar_grupos = 0.60 #65\n","similarity_minima_pra_encaixar_itens_excluidos_no_final = 0.85 #80\n","\n","unidades = ['x','mm','m','cm','ml','g','mg','kg','unidade','unidades','polegada','polegadas','grama','gramas','gb','mb','l','lr','lt','litro','litros','mts','un','mgml','w','hz','v','gr','lts','lonas','cores','mcg']\n","path_drive = 'drive/My Drive/Residencia/'"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"AvzYNNccz6nc","colab_type":"text"},"source":["###Descriptions Cleaning"]},{"cell_type":"code","metadata":{"id":"3Atof5NToHB0","colab_type":"code","colab":{}},"source":["##########################\n","# Para alcool\n","##########################\n","df_covid = pd.read_csv('ncms_covid.csv')\n","df = pd.read_csv('notas.csv')\n","\n","ncms_covid = df_covid['ncm'].values.tolist()\n","\n","df = df[df['ncm'].apply(lambda ncm: ncm in ncms_covid)]\n","\n","#print(collections.Counter(df['ncm'].values.tolist()).most_common(10))\n","ncms_selecionados = [22072019, 38089429, 63079010]\n","#most_common = [ncm for ncm, qtd in collections.Counter(df['ncm'].values.tolist()).most_common(2)]\n","\n","df = df[df['ncm'].apply(lambda ncm: ncm in ncms_selecionados)]\n","df = df[df['xprod'].apply(lambda xprod: 'alcool' in xprod)]\n","#df['xprod'].iloc[np.random.randint(len(df),size=50)]\n","#df['xprod'].iloc[np.random.randint(len(df),size=50)].apply(len)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3hrfHWh6H8HU","colab_type":"code","colab":{}},"source":["##########################\n","# Para testes\n","##########################\n","#df = pd.read_csv('notas.csv')\n","#testes: 30021590 38220090 30021229\n","#df = df[df['ncm'] == 30021229]\n","'''\n","df = df[df['xprod'].apply(lambda xprod: 'sars' in xprod.lower() or 'covid' in xprod.lower() or 'corona' in xprod)]\n","df= df[~df['xprod'].str.contains('mascara')]\n","df= df[~df['xprod'].str.contains('alcool')]\n","df= df[~df['xprod'].str.contains('doacao')]\n","df= df[~df['xprod'].str.contains('camisa')]\n","'''\n","#df= df[df['xprod'].str.contains('polimerase ')]\n","#print(len(df))\n","\n","#df['xprod'].iloc[np.random.randint(len(df),size=50)]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"EyeBnXFcoRB2","colab_type":"code","colab":{}},"source":["nltk.download('stopwords')\n","\n","pt_stopwords = set(nltk.corpus.stopwords.words(\"portuguese\"))\n","stopwords = set(list(punctuation))\n","stopwords.add('embalagem')\n","stopwords.add('emb')\n","stopwords.add('tipo')\n","stopwords.add('inpm')\n","stopwords.add('jalles')\n","stopwords.add('becker')\n","pt_stopwords.update(stopwords)\n","\n","#limpa sentencas retirando stopwords, pontuacao e deixa minusculo.\n","df['xprod'] = [ ' '.join([word.lower() for word in descr.split() if word.lower() not in pt_stopwords]) for descr in df['xprod'].astype(str)]\n","#insere espaco apos / e -, pra no final nao ficar palavras assim: csolucao, ptexto (originais eram c/solucao, p-texto)\n","df['xprod'] = df['xprod'].apply(lambda descr: re.sub(r'/|-',r' ',descr))\n","#retira pontuacao, /, etc:\n","df['xprod'] = df['xprod'].apply(lambda x: x.translate(str.maketrans('', '', string.punctuation)))\n","#insere espaco apos numero e letra (separa unidades de medida:) ex.: 500ml vs 100ml vs 500mg\n","df['xprod'] = df['xprod'].apply(lambda x: re.sub(r'(\\d{1})(\\D)',r'\\1 \\2',x))\n","#insere espaco apos letra e numero ex.:c100 pc50\n","df['xprod'] = df['xprod'].apply(lambda x: re.sub(r'(\\D{1})(\\d)',r'\\1 \\2',x))\n","#chars de tamanho inferior a 2 ficam apenas se for unidades\n","#df['xprod'] = df['xprod'].apply(lambda x: re.sub(r'\\b[a-zA-Z]\\b',r'',x))\n","df['xprod'] = df['xprod'].apply(lambda x: ' '.join([word for word in x.split() if len(word) > 2 or word in unidades or word.isdigit()]))\n","#retira espacos duplicados\n","df['xprod'] = df['xprod'].apply(lambda x: re.sub(r' +',r' ', x))\n","#remove numero de lote\n","df['xprod'] = df['xprod'].apply(lambda x: re.sub(r'lote \\d+',r'', x))\n","#df['xprod'] = df['xprod'].apply(lambda x: ' '.join(x.split()[:-4]) if 'lotes' in x else x)\n","#muda l e lts e litros por lt\n","df['xprod'] = df['xprod'].apply(lambda x: re.sub(r' l ',r' lt ', x))\n","df['xprod'] = df['xprod'].apply(lambda x: re.sub(r' lts ',r' lt ', x))\n","df['xprod'] = df['xprod'].apply(lambda x: re.sub(r' litros ',r' lt ', x))\n","df['xprod'] = df['xprod'].apply(lambda x: re.sub(r' litro ',r' lt ', x))\n","#muda l e lts por lt se for no final de frase\n","df['xprod'] = df['xprod'].apply(lambda x: x  if x.split()[-1] != 'l' else re.sub(r' l',r' lt', x))\n","df['xprod'] = df['xprod'].apply(lambda x: x  if x.split()[-1] != 'lts' else re.sub(r'lts',r'lt', x))\n","df['xprod'] = df['xprod'].apply(lambda x: x  if x.split()[-1] != 'litro' else re.sub(r'litro',r'lt', x))\n","df['xprod'] = df['xprod'].apply(lambda x: x  if x.split()[-1] != 'litros' else re.sub(r'litros',r'lt', x))\n","#remove marcas\n","df['xprod'] = df['xprod'].apply(lambda x: re.sub(r'marca\\s(\\w+)',r'', x))\n","df['xprod'] = df['xprod'].apply(lambda x: re.sub(r'fabricante\\s(\\w+)',r'', x))\n","#retira espaco no inicio da frase\n","df['xprod'] = df['xprod'].apply(lambda x: x.strip())\n","#retira acentos:\n","df['xprod'] = df['xprod'].apply(lambda x: unidecode.unidecode(x))\n","# remove zeros a esquerda de numeros (02 litros, 05, etc.)\n","df['xprod'] = df['xprod'].apply(lambda x: ' '.join([word.lstrip('0') for word in x.split()] ) )\n","# remove 'x', pra não diferenciar pneu 275 80 de 275 x 80:\n","df['xprod'] = df['xprod'].apply(lambda x: ' '.join([word for word in x.split() if word != 'x']))\n","#se primeira palavra for numero apaga ao invés joga pro final\n","#df['xprod'] = df['xprod'].apply(lambda x: ' '.join(x.split()[1:] + [x.split()[0]]) if ((len(x) > 1) and (x.split()[0].isdigit()) ) else x)\n","df['xprod'] = df['xprod'].apply(lambda x: ' '.join(x.split()[1:]) if ((len(x) > 1) and (x.split()[0].isdigit()) ) else x)\n","#substitui 1000ml por 1l \n","df['xprod'] = df['xprod'].apply(lambda x: re.sub(r'1000 ml',r'1 lt',x))\n","df['xprod'] = df['xprod'].apply(lambda x: re.sub(r'5000 ml',r'5 lt',x))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DR61IzGoo6sJ","colab_type":"text"},"source":["###Pre processing"]},{"cell_type":"code","metadata":{"id":"7rzCd_TcoJDV","colab_type":"code","colab":{}},"source":["sentences = [descr.split()[:qtd_palavras] for descr in df['xprod']]\n","\n","'''\n","model = FastText(size=qtd_dimensoes, min_count=1)  \n","model.build_vocab(sentences=sentences)\n","model.train(sentences=sentences, total_examples=len(sentences), epochs=10)\n","'''\n","model2 = Word2Vec(sentences,size=qtd_dimensoes, min_count=1, workers=-1)\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"R8EiZ2sL5kP5","colab_type":"text"},"source":["####Aplicando pesos"]},{"cell_type":"markdown","metadata":{"id":"YK8l8S79y32r","colab_type":"text"},"source":["#####Fasttext"]},{"cell_type":"code","metadata":{"id":"nuvHznJrTR9w","colab_type":"code","colab":{}},"source":["'''\n","#Conversao word embeddings to sentence embedding, com pesos\n","#dando peso maior pras primeiras palavras, peso decrescente ateh o final, numeros com mesmo peso da primeira palavra:\n","doc_vectors = {}\n","for number, sent in enumerate(tqdm(sentences)):\n","    if len(sent) == 0:\n","        doc_vectors[number] = np.zeros(qtd_dimensoes,)\n","    elif len(sent) == 1:\n","        doc_vectors[number] = model[sent[0]]\n","    elif len(sent) > 1:\n","        pesos = np.array(range(len(sent))[::]) + 1\n","        # agora com pesos 1/x - tem que ser na ordem 1,2,..., os menores numeros dao maiores pesos\n","        pesos = 1 / pesos \n","        media = []\n","        divisao = 0\n","        counter = 0\n","        for word in sent:\n","            if word.isdigit():\n","                #se eh digit, atribui peso NO 3/4 da faixa entre o peso da primeira e da ultima palavra. \n","                #Mesmo peso pra todos os numeros, mais importante que palavras do fim, menos importante que palavras do inicio.\n","                media.append(model.wv[word] * ((pesos[0]+pesos[-1])*(1/4))) \n","                divisao += ((pesos[0]+pesos[-1])*(1/4))\n","            else:\n","                media.append(model.wv[word] * pesos[counter])\n","                divisao += pesos[counter]\n","            counter += 1\n","        doc_vectors[number] = np.array(media).sum(axis=0) / divisao #media de tudo\n","'''"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"65O1y3AVzGj8","colab_type":"text"},"source":["#####Word2vec"]},{"cell_type":"code","metadata":{"id":"B2oAJKYWHCzy","colab_type":"code","colab":{}},"source":["doc_vectors2 = {}\n","\n","for number, sent in enumerate(tqdm(sentences)):\n","    #agora dando peso maior pras primeiras palavras, peso decrescente ateh o final, numeros com mesmo peso da primeira palavra:\n","    if len(sent) == 0:\n","        doc_vectors2[number] = np.zeros(qtd_dimensoes,)\n","    elif len(sent) == 1:\n","        doc_vectors2[number] = model2.wv[sent[0]]\n","    elif len(sent) > 1:\n","        pesos = np.array(range(len(sent))[::]) + 1\n","        pesos = 1 / pesos\n","        media = []\n","        divisao = 0\n","        counter = 0\n","        for word in sent:\n","            if word.isdigit():\n","                media.append(model2.wv[word] * ((pesos[0]+pesos[-1])*(1/2)) )\n","                divisao += ((pesos[0]+pesos[-1])*(1/2))\n","            else:\n","                media.append(model2.wv[word] * pesos[counter])\n","                divisao += pesos[counter]\n","            counter += 1\n","        doc_vectors2[number] = np.array(media).sum(axis=0) / divisao #media de tudo"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"iKFu7hW4UxiT","colab_type":"code","colab":{}},"source":["doc_vectors2 = pd.DataFrame(doc_vectors2).T\n","doc_vectors3 = doc_vectors2\n","doc_vectors2 = doc_vectors2.set_index(df.index) \n","\n","#grava(doc_vectors2, path_drive, 'word2vec_alcool.pkl')\n","\n","scaler = StandardScaler()\n","doc_vectors_std_df = pd.DataFrame(scaler.fit_transform(doc_vectors2),index=doc_vectors2.index,columns=doc_vectors2.columns)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rSjnIytYpUBg","colab_type":"text"},"source":["####Redução de dimensão"]},{"cell_type":"code","metadata":{"id":"F8T8xn6np339","colab_type":"code","colab":{}},"source":["#n_neighbors=20\n","umap_redux = umap.UMAP(n_components=qtd_dimensoes_umap, random_state=999, metric='cosine')\n","umap_redux.fit(doc_vectors_std_df)\n","\n","doc_vectors_std_df_umap = umap_redux.transform(X=doc_vectors_std_df)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"c0SYzfZEz1hZ","colab_type":"text"},"source":["###Clustering"]},{"cell_type":"code","metadata":{"id":"U3xRCr2uy_SG","colab_type":"code","colab":{}},"source":["#clustering = hdbscan.HDBSCAN(min_cluster_size=tamanho_minimo_pra_formar_grupo,min_samples=45,prediction_data=True,core_dist_n_jobs=-1).fit(doc_vectors_std_df_umap)\n","clustering = abre(path_drive, 'HDBSCAN_modelo.pkl')\n","\n","df['grupo'] = clustering.labels_"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WRoFltN-0l3O","colab_type":"code","colab":{}},"source":["threshold = pd.Series(clustering.outlier_scores_).quantile(quantile_a_retirar_outliers_dbscan)\n","outliers = np.where(clustering.outlier_scores_ > threshold)[0]\n","df.iloc[outliers,df.columns.get_loc('grupo')] = -2"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"r2BIbixl1xZM","colab_type":"code","colab":{}},"source":["grupos = np.unique(clustering.labels_)\n","grupos = [grupo for grupo in grupos if grupo >= 0]\n","\n","print('qtd de grupos:',len(grupos))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-Gtd4VViWs81","colab_type":"text"},"source":["###Cluster Cleaning"]},{"cell_type":"code","metadata":{"id":"Tv5mrKua4oHK","colab_type":"code","colab":{}},"source":["#########################################\n","# EXCLUI sentences outliers pelo texto pelos exemplars\n","# FAZ cosine distance pra cada sentence dentro dos grupos\n","#########################################\n","\n","#pontos mais representativos de cada grupo (como hdbscan aceita formato aleatorio de cada cluster, nao tem como usar centroide)\n","exemplars = []\n","for exemplar in clustering.exemplars_:\n","    exemplars.append(np.mean(exemplar,axis=0))\n","exemplars_df = pd.DataFrame(exemplars,index=range(len(grupos)))\n","\n","map_grupos_exemplars = {}\n","df_temp = pd.DataFrame(columns=['sims'])\n","\n","for grupo in grupos[:]:\n","    df2 = df[df['grupo'] == grupo]\n","    indexes = df2.index\n","    grupo_vectors = pd.DataFrame(doc_vectors_std_df_umap,index=df.index).loc[indexes]\n","    \n","    grupo_do_exemplar = pd.Series(cosine_similarity(grupo_vectors.mean(axis=0).values.reshape(1,-1),exemplars_df)[0]).sort_values(ascending=False).index[0]\n","    map_grupos_exemplars[grupo] = grupo_do_exemplar\n","\n","    sims = cosine_similarity(grupo_vectors,exemplars[grupo_do_exemplar].reshape(1,-1))\n","\n","    df2['sims'] = sims\n","    df_temp = df_temp.append(df2[['sims']])\n","\n","#passa resultados pro df principal:\n","df['sims'] = df_temp\n","df['sims'] = df['sims'].replace(np.nan,-1)\n","\n","#retira quem tem similaridade negativa - eh um bom parametro.\n","df['grupo2'] = np.where(df['sims'] < 0, -1, df['grupo'])\n","\n","grupos = df['grupo2'].unique()\n","grupos = [grupo for grupo in grupos if grupo >= 0]\n","\n","print('qtd de grupos:',len(grupos))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"yJE_QTbw37Ym","colab_type":"code","colab":{}},"source":["##########################################\n","# Se a 1a palavra nao for a mesma em X% do grupo, exclui o grupo, eh muito heterogeneo\n","##########################################\n","df['grupo3'] = df['grupo2']\n","grupos = sorted(df['grupo3'].unique())\n","grupos = [grupo for grupo in grupos if grupo >=0] #tirar os -1, -2, etc.\n","\n","grupos_homogeneos = []\n","\n","for grupo in tqdm(grupos):\n","    df2 = df[df['grupo3'] == grupo]\n","    if len(df2) > 0:\n","        if ( df2['xprod'].apply(lambda x: x.split()[0] if (len(x.split()) > 0) else np.random.random()).value_counts().iloc[0] / len(df2) ) > percentual_primeira_palavra_igual_pra_considerar_grupo_homogeneo:\n","            grupos_homogeneos.append(grupo)\n","\n","df['grupo4'] = df['grupo3'].isin(grupos_homogeneos)\n","df['grupo4'] = np.where(df['grupo4'], df['grupo3'], -1)\n","\n","grupos = sorted(df['grupo4'].unique())\n","grupos = [grupo for grupo in grupos if grupo >=0] \n","\n","print('qtd de grupos:',len(grupos))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"n6But7J-NbEr","colab_type":"code","colab":{}},"source":["############################################################################\n","# EXCLUI GRUPOS NAO HOMOGENEOS - pela contagem de palavras diferentes! \n","############################################################################\n","df['qtd_palavras_diferentes'] = df['xprod'].apply(lambda x: len(set( [item for sublist in [sent.split()[:qtd_palavras] for sent in x] for item in sublist] )))\n","qtd_palavras_por_grupo = df.groupby('grupo4')['qtd_palavras_diferentes'].median()\n","qtd_palavras_por_grupo = qtd_palavras_por_grupo.sort_values()\n","qtd_max_palavras_diferentes_no_grupo = int(qtd_palavras_por_grupo.quantile(quantile_a_retirar_quantidade_palavras_diferentes_no_grupo))\n","print('Quantidade maxima de palavras diferentes aceita por grupo:', qtd_max_palavras_diferentes_no_grupo)\n","\n","df['qtd_median_palavras_dif_grupo'] = df['grupo4'].map(qtd_palavras_por_grupo)\n","\n","df['grupo5'] = np.where(df['qtd_median_palavras_dif_grupo'] > qtd_max_palavras_diferentes_no_grupo, -1, df['grupo4'])\n","\n","grupos = df['grupo5'].unique()\n","grupos = [grupo for grupo in grupos if grupo >=0]\n","\n","print('qtd de grupos:',len(grupos))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9TlqI_1gNKQ4","colab_type":"code","colab":{}},"source":["df['grupo6'] = df['grupo5']\n","#df['grupo6'] = df['grupo4']\n","############################################################################\n","# EXCLUI GRUPOS que tem MUITOS NUMEROS DIFERENTES NAS SENTENCES\n","############################################################################\n","quantile_a_retirar_numeros_diferentes_no_grupo = 0.95\n","\n","df['qtd_numeros_diferentes'] = df['xprod'].apply(lambda x: len(set( [item for sublist in [sent.split()[:qtd_palavras] for sent in x] for item in sublist if item.isdigit()   ] ) ))\n","df['qtd_numeros_diferentes'] = np.where(df['qtd_numeros_diferentes'] == 0, 0, df['qtd_numeros_diferentes']-1)\n","qtd_numeros_por_grupo = df.groupby('grupo4')['qtd_numeros_diferentes'].median()\n","qtd_numeros_por_grupo = qtd_numeros_por_grupo[qtd_numeros_por_grupo > 0]\n","qtd_numeros_por_grupo = qtd_numeros_por_grupo.sort_values()\n","qtd_max_numeros_diferentes_no_grupo = int(qtd_numeros_por_grupo.quantile(quantile_a_retirar_numeros_diferentes_no_grupo))\n","print('Quantidade maxima de numeros diferentes aceita por grupo:', qtd_max_numeros_diferentes_no_grupo)\n","\n","df['qtd_median_numeros_dif_grupo'] = df['grupo4'].map(qtd_numeros_por_grupo)\n","\n","df['grupo7'] = np.where(df['qtd_median_numeros_dif_grupo'] > qtd_max_numeros_diferentes_no_grupo, -1, df['grupo6'])\n","\n","grupos = df['grupo7'].unique()\n","grupos = [grupo for grupo in grupos if grupo >=0] \n","\n","print('qtd de grupos:',len(grupos))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"vY-YyIdU5oUo","colab_type":"code","colab":{}},"source":["doc_vectors3 = doc_vectors3.set_index(df.index) \n","doc_vectors_grupos = {}\n","\n","for grupo in tqdm(grupos):\n","    indices = df[df['grupo7'] == grupo].index\n","    doc_vectors_grupos[grupo] = doc_vectors3.loc[indices]\n","    doc_vectors_grupos[grupo] = doc_vectors_grupos[grupo].mean(axis=0)   \n","\n","doc_vectors_grupos = pd.DataFrame(doc_vectors_grupos).T\n","doc_vectors_grupos_std = pd.DataFrame(scaler.transform(doc_vectors_grupos),index=doc_vectors_grupos.index,columns=doc_vectors_grupos.columns)\n","\n","grupos_similarities = cosine_similarity(doc_vectors_grupos_std)\n","grupos_similarities = pd.DataFrame(grupos_similarities,index=doc_vectors_grupos.index,columns=doc_vectors_grupos.index)\n","\n","#junta os grupos:\n","grupos_similares = []\n","for grupo in grupos_similarities:\n","    agrupar_df = grupos_similarities[grupo].sort_values(ascending=False)\n","    agrupar_df = agrupar_df[agrupar_df >= similarity_minima_pra_juntar_grupos]\n","    grupos_similares.append(list(agrupar_df.index))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NJ9VsWtschDn","colab_type":"code","colab":{}},"source":["novo_grupo = 0\n","mapeamento_grupos = {}\n","for mini_grupo in grupos_similares:\n","    if len(mini_grupo) == 1:\n","        mapeamento_grupos[mini_grupo[0]] = novo_grupo\n","    else:\n","        for grupo in mini_grupo:\n","            if grupo not in mapeamento_grupos.keys():\n","                for mini_grupo2 in grupos_similares:\n","                    if grupo in mini_grupo2:\n","                        mapeamento_grupos[grupo] = novo_grupo\n","                        for grupo2 in mini_grupo2:\n","                            if grupo2 not in mapeamento_grupos.keys():\n","                                mapeamento_grupos[grupo2] = novo_grupo\n","    novo_grupo += 1\n","\n","df['grupo9'] = df['grupo7'].map(mapeamento_grupos)\n","df['grupo9'] = df['grupo9'].fillna(-1)\n","df['grupo9'] = df['grupo9'].astype(int)\n","\n","grupos = df['grupo9'].unique()\n","grupos = [grupo for grupo in grupos if grupo >=0] \n","\n","print('qtd de grupos:',len(grupos))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"HP3LFYlnvMc1","colab_type":"code","colab":{}},"source":["###############################################\n","# Repassar as sentences do grupo -1 e tentar encaixá-las nos grupos formados\n","###############################################\n","\n","excluidos_index =  df[df['grupo9'] == -1].index\n","incluidos_index =  df[df['grupo9'] >= 0].index\n","\n","doc_vectors_excluidos = doc_vectors2.loc[excluidos_index]\n","doc_vectors_incluidos = doc_vectors2.loc[incluidos_index]\n","\n","doc_vectors_grupos_finais = {}\n","for grupo in grupos:\n","    df2 = df[df['grupo9'] == grupo]\n","    doc_vectors_grupos_finais[grupo] = doc_vectors_incluidos.loc[df2.index]\n","    doc_vectors_grupos_finais[grupo] = doc_vectors_grupos_finais[grupo].values.mean(axis=0)\n","\n","doc_vectors_grupos_finais = pd.DataFrame(doc_vectors_grupos_finais).T\n","compara = cosine_similarity(doc_vectors_excluidos.loc[excluidos_index],doc_vectors_grupos_finais.values)\n","compara = pd.DataFrame(compara,index=excluidos_index, columns=grupos)\n","similarity_do_grupo_mais_parecido = compara.max(axis=1)\n","grupo_mais_parecido = compara.idxmax(axis=1)\n","\n","encaixar_excluidos = pd.Series( np.where(similarity_do_grupo_mais_parecido >= similarity_minima_pra_encaixar_itens_excluidos_no_final, grupo_mais_parecido, -1), index= similarity_do_grupo_mais_parecido.index)\n","df['grupo10'] = encaixar_excluidos\n","df['grupo10'] = df['grupo10'].fillna(-1)\n","df['grupo10'] = np.where(df['grupo10'] == -1, df['grupo9'], df['grupo10'])\n","\n","grupos = df['grupo10'].unique()\n","grupos = [grupo for grupo in grupos if grupo >=0] \n","\n","print('qtd de grupos:',len(grupos))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qRwxDQHfU7xK","colab_type":"text"},"source":["###Resultados"]},{"cell_type":"code","metadata":{"id":"MbRbEd1mKMR2","colab_type":"code","colab":{}},"source":["#print_exemplos_grupos_v2_aleatorio(df=df,grupos=grupos,grupox='grupo7',cols=['xprod', 'vuncom'],qtd_palavras=qtd_palavras,unidades=unidades)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YR4razwi-Wzk","colab_type":"code","colab":{}},"source":["#grupos_similares"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Od-dW55mN3h-","colab_type":"code","colab":{}},"source":["#grupos_similarities"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6lqdcH78OEQt","colab_type":"code","colab":{}},"source":["print_exemplos_grupos_v2_aleatorio(df=df,grupos=grupos,grupox='grupo10',cols=['xprod', 'vuncom'],qtd_palavras=qtd_palavras,unidades=unidades)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"DNcow1jHQIAf","colab_type":"code","colab":{}},"source":["values = [-1, -2] #verificar os que ficaram de fora (-1) e foram considerados outliers (-2)\n","out_inicio = df[df.grupo.isin(values)]\n","out_fim = df[df['grupo10']==-1]\n","\n","resu = (len(out_inicio)+len(out_fim))/(len(df['ncm'].values.tolist()))*100\n","\n","print(f'Ficaram fora no início: {len(out_inicio)}')\n","print(f'Ficaram fora no fim: {len(out_fim)}')\n","print(f'Total de registros de fora (%): {resu}')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"cPz0phDWj7BX","colab_type":"code","colab":{}},"source":["out_inicio.to_excel('ficaram_fora_inicio.xlsx')\n","out_fim.to_excel('ficaram_fora_fim.xlsx')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hN4c1K2diMSd","colab_type":"code","colab":{}},"source":["df_precos, df_list_grupos = calc_preco(df=df,grupos=grupos,grupox='grupo10')\n","save_groups(df_list_grupos)\n","df_precos"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Z4t-P_MI1SOE","colab_type":"code","colab":{}},"source":["df_descrs_unicas = descricoes_grupos(df=df,grupos=grupos,grupox='grupo10')\n","#df_descrs_unicas[df_descrs_unicas['Grupo'] == 1][:50]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"HfMwQJcgUJP8","colab_type":"code","colab":{}},"source":["#grava(clustering, path_drive, 'HDBSCAN_modelo.pkl')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2hPTM3QohYow","colab_type":"text"},"source":["###Predição \n"]},{"cell_type":"code","metadata":{"id":"RoImQEb0muFg","colab_type":"code","colab":{}},"source":["df_teste = pd.read_csv('notas.csv')\n","\n","ncms_covid = df_covid['ncm'].values.tolist()\n","\n","ncms_selecionados = [22072019, 38089429, 63079010]\n","\n","df_teste = df_teste[df_teste['ncm'].apply(lambda ncm: ncm in ncms_selecionados)]\n","df_teste = df_teste[df_teste['xprod'].apply(lambda xprod: 'alcool' in xprod)]\n","df_teste = df_teste[:300]\n","df_teste['grupo9'] = -1"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4AMaxn22hcks","colab_type":"code","colab":{}},"source":["nltk.download('stopwords')\n","\n","pt_stopwords = set(nltk.corpus.stopwords.words(\"portuguese\"))\n","stopwords = set(list(punctuation))\n","stopwords.add('embalagem')\n","stopwords.add('emb')\n","stopwords.add('tipo')\n","stopwords.add('inpm')\n","stopwords.add('jalles')\n","stopwords.add('becker')\n","pt_stopwords.update(stopwords)\n","\n","#limpa sentencas retirando stopwords, pontuacao e deixa minusculo.\n","df_teste['xprod'] = [ ' '.join([word.lower() for word in descr.split() if word.lower() not in pt_stopwords]) for descr in df_teste['xprod'].astype(str)]\n","#insere espaco apos / e -, pra no final nao ficar palavras assim: csolucao, ptexto (originais eram c/solucao, p-texto)\n","df_teste['xprod'] = df_teste['xprod'].apply(lambda descr: re.sub(r'/|-',r' ',descr))\n","#retira pontuacao, /, etc:\n","df_teste['xprod'] = df_teste['xprod'].apply(lambda x: x.translate(str.maketrans('', '', string.punctuation)))\n","#insere espaco apos numero e letra (separa unidades de medida:) ex.: 500ml vs 100ml vs 500mg\n","df_teste['xprod'] = df_teste['xprod'].apply(lambda x: re.sub(r'(\\d{1})(\\D)',r'\\1 \\2',x))\n","#insere espaco apos letra e numero ex.:c100 pc50\n","df_teste['xprod'] = df_teste['xprod'].apply(lambda x: re.sub(r'(\\D{1})(\\d)',r'\\1 \\2',x))\n","#chars de tamanho inferior a 2 ficam apenas se for unidades\n","#df_teste['xprod'] = df_teste['xprod'].apply(lambda x: re.sub(r'\\b[a-zA-Z]\\b',r'',x))\n","df_teste['xprod'] = df_teste['xprod'].apply(lambda x: ' '.join([word for word in x.split() if len(word) > 2 or word in unidades or word.isdigit()]))\n","#retira espacos duplicados\n","df_teste['xprod'] = df_teste['xprod'].apply(lambda x: re.sub(r' +',r' ', x))\n","#remove numero de lote\n","df_teste['xprod'] = df_teste['xprod'].apply(lambda x: re.sub(r'lote \\d+',r'', x))\n","df_teste['xprod'] = df_teste['xprod'].apply(lambda x: ' '.join(x.split()[:-4]) if 'lotes' in x else x)\n","#muda l e lts e litros por lt\n","df_teste['xprod'] = df_teste['xprod'].apply(lambda x: re.sub(r' l ',r' lt ', x))\n","df_teste['xprod'] = df_teste['xprod'].apply(lambda x: re.sub(r' lts ',r' lt ', x))\n","df_teste['xprod'] = df_teste['xprod'].apply(lambda x: re.sub(r' litros ',r' lt ', x))\n","df_teste['xprod'] = df_teste['xprod'].apply(lambda x: re.sub(r' litro ',r' lt ', x))\n","#muda l e lts por lt se for no final de frase\n","df_teste['xprod'] = df_teste['xprod'].apply(lambda x: x  if x.split()[-1] != 'l' else re.sub(r' l',r' lt', x))\n","df_teste['xprod'] = df_teste['xprod'].apply(lambda x: x  if x.split()[-1] != 'lts' else re.sub(r'lts',r'lt', x))\n","df_teste['xprod'] = df_teste['xprod'].apply(lambda x: x  if x.split()[-1] != 'litro' else re.sub(r'litro',r'lt', x))\n","df_teste['xprod'] = df_teste['xprod'].apply(lambda x: x  if x.split()[-1] != 'litros' else re.sub(r'litros',r'lt', x))\n","#remove marcas\n","df_teste['xprod'] = df_teste['xprod'].apply(lambda x: re.sub(r'marca\\s(\\w+)',r'', x))\n","df_teste['xprod'] = df_teste['xprod'].apply(lambda x: re.sub(r'fabricante\\s(\\w+)',r'', x))\n","#retira espaco no inicio da frase\n","df_teste['xprod'] = df_teste['xprod'].apply(lambda x: x.strip())\n","#retira acentos:\n","df_teste['xprod'] = df_teste['xprod'].apply(lambda x: unidecode.unidecode(x))\n","# remove zeros a esquerda de numeros (02 litros, 05, etc.)\n","df_teste['xprod'] = df_teste['xprod'].apply(lambda x: ' '.join([word.lstrip('0') for word in x.split()] ) )\n","# remove 'x', pra não diferenciar pneu 275 80 de 275 x 80:\n","df_teste['xprod'] = df_teste['xprod'].apply(lambda x: ' '.join([word for word in x.split() if word != 'x']))\n","#se primeira palavra for numero apaga ao invés joga pro final\n","#df_teste['xprod'] = df_teste['xprod'].apply(lambda x: ' '.join(x.split()[1:] + [x.split()[0]]) if ((len(x) > 1) and (x.split()[0].isdigit()) ) else x)\n","df_teste['xprod'] = df_teste['xprod'].apply(lambda x: ' '.join(x.split()[1:]) if ((len(x) > 1) and (x.split()[0].isdigit()) ) else x)\n","#substitui 1000ml por 1l \n","df_teste['xprod'] = df_teste['xprod'].apply(lambda x: re.sub(r'1000 ml',r'1 lt',x))\n","df_teste['xprod'] = df_teste['xprod'].apply(lambda x: re.sub(r'5000 ml',r'5 lt',x))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"igOBtFkKiTXB","colab_type":"code","colab":{}},"source":["sentences_teste = [descr.split()[:qtd_palavras] for descr in df_teste['xprod']]\n","\n","model2 = Word2Vec(sentences_teste,size=qtd_dimensoes, min_count=1, workers=-1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0oAjXIfXioEh","colab_type":"code","colab":{}},"source":["doc_vectors_teste = {}\n","for number, sent in enumerate(sentences_teste):\n","    #agora dando peso maior pras primeiras palavras, peso decrescente ateh o final, numeros com mesmo peso da primeira palavra:\n","    if len(sent) == 0:\n","        doc_vectors_teste[number] = np.zeros(qtd_dimensoes,)\n","    elif len(sent) == 1:\n","        doc_vectors_teste[number] = model2.wv[sent[0]]\n","    elif len(sent) > 1:\n","        pesos = np.array(range(len(sent))[::]) + 1\n","        pesos = 1 / pesos\n","        media = []\n","        divisao = 0\n","        counter = 0\n","        for word in sent:\n","            if word.isdigit():\n","                media.append(model2.wv[word] * ((pesos[0]+pesos[-1])*(1/2)) )\n","                divisao += ((pesos[0]+pesos[-1])*(1/2))\n","            else:\n","                media.append(model2.wv[word] * pesos[counter])\n","                divisao += pesos[counter]\n","            counter += 1\n","        doc_vectors_teste[number] = np.array(media).sum(axis=0) / divisao #media de tudo"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"nHJ4s8iajS2x","colab_type":"code","colab":{}},"source":["doc_vectors_teste = pd.DataFrame(doc_vectors_teste).T\n","doc_vectors_teste = doc_vectors_teste.set_index(df_teste.index) \n","\n","scaler = StandardScaler()\n","doc_vectors_std_df = pd.DataFrame(scaler.fit_transform(doc_vectors_teste),index=doc_vectors_teste.index,columns=doc_vectors_teste.columns)\n","\n","#n_neighbors=20\n","umap_redux = umap.UMAP(n_components=qtd_dimensoes_umap, random_state=999, metric='cosine')\n","umap_redux.fit(doc_vectors_std_df)\n","\n","doc_vectors_std_df_umap = umap_redux.transform(X=doc_vectors_std_df)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Kzctdr65ja_g","colab_type":"code","colab":{}},"source":["###############################################\n","# Repassar as sentences do grupo -1 e tentar encaixá-las nos grupos formados\n","###############################################\n","\n","#excluidos_index =  df[df['grupoteste'] == -1].index\n","excluidos_index = doc_vectors_teste.index\n","incluidos_index =  df[df['grupo9'] >= 0].index\n","\n","doc_vectors_excluidos = doc_vectors_teste\n","doc_vectors_incluidos = doc_vectors2.loc[incluidos_index]\n","\n","doc_vectors_grupos_finais = {}\n","for grupo in grupos:\n","    df2 = df[df['grupo9'] == grupo]\n","    doc_vectors_grupos_finais[grupo] = doc_vectors_incluidos.loc[df2.index]\n","    doc_vectors_grupos_finais[grupo] = doc_vectors_grupos_finais[grupo].values.mean(axis=0)\n","\n","doc_vectors_grupos_finais = pd.DataFrame(doc_vectors_grupos_finais).T\n","\n","compara = cosine_similarity(doc_vectors_excluidos,doc_vectors_grupos_finais.values)\n","compara = pd.DataFrame(compara,index=excluidos_index, columns=grupos)\n","similarity_do_grupo_mais_parecido = compara.max(axis=1)\n","grupo_mais_parecido = compara.idxmax(axis=1)\n","\n","encaixar_excluidos = pd.Series( np.where(similarity_do_grupo_mais_parecido >= similarity_minima_pra_encaixar_itens_excluidos_no_final, grupo_mais_parecido, -1), index= similarity_do_grupo_mais_parecido.index)\n","df_teste['grupo10'] = encaixar_excluidos\n","df_teste['grupo10'] = df_teste['grupo10'].fillna(-1)\n","df_teste['grupo10'] = np.where(df_teste['grupo10'] == -1, df_teste['grupo9'], df_teste['grupo10'])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"OL9k9uetroLn","colab_type":"code","colab":{}},"source":["print_exemplos_grupos_v2_aleatorio(df=df_teste,grupos=grupos,grupox='grupo10',cols=['xprod', 'vuncom'],qtd_palavras=qtd_palavras,unidades=unidades)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0tQwJ3I8xbxa","colab_type":"text"},"source":["###Predição (não funciona)\n"]},{"cell_type":"code","metadata":{"id":"-OCVGg9p9Rgf","colab_type":"code","colab":{}},"source":["test_values = ['alcool gel 70 1l', 'alcool etilico 1l', 'alcool gel 70 1l', 'alcool etilico 1l','alcool gel 70 1l', 'alcool etilico 1l']\n","\n","model3 = Word2Vec(test_values,size=qtd_dimensoes, min_count=1, workers=-1)\n","\n","doc_vectors4 = {}\n","\n","for number, sent in enumerate(tqdm(test_values)):\n","    #agora dando peso maior pras primeiras palavras, peso decrescente ateh o final, numeros com mesmo peso da primeira palavra:\n","    if len(sent) == 0:\n","        doc_vectors4[number] = np.zeros(qtd_dimensoes,)\n","    elif len(sent) == 1:\n","        doc_vectors4[number] = model3.wv[sent[0]]\n","    elif len(sent) > 1:\n","        pesos = np.array(range(len(sent))[::]) + 1\n","        pesos = 1 / pesos\n","        media = []\n","        divisao = 0\n","        counter = 0\n","        for word in sent:\n","            #### AGORA O MODEL EH W2V E O PESO EH DOBRADO PRA DIGITS:\n","            if word.isdigit():\n","                media.append(model3.wv[word] * ((pesos[0]+pesos[-1])*(1/2)) )\n","                divisao += ((pesos[0]+pesos[-1])*(1/2))\n","            else:\n","                media.append(model3.wv[word] * pesos[counter])\n","                divisao += pesos[counter]\n","            counter += 1\n","        doc_vectors4[number] = np.array(media).sum(axis=0) / divisao #media de tudo"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"IiOGlKs30R0Z","colab_type":"code","colab":{}},"source":["df_teste = pd.DataFrame(test_values)\n","\n","doc_vectors4 = pd.DataFrame(doc_vectors4).T\n","doc_vectors4 = doc_vectors4.set_index(df_teste.index) \n","\n","scaler = StandardScaler()\n","doc_vectors4_std_df = pd.DataFrame(scaler.fit_transform(doc_vectors4),index=doc_vectors4.index,columns=doc_vectors4.columns)\n","\n","umap_redux = umap.UMAP(n_components=qtd_dimensoes_umap, random_state=999, init='random', metric='cosine') #alteração no init\n","umap_redux.fit(doc_vectors4_std_df)\n","\n","doc_vectors4_std_df_umap = umap_redux.transform(X=doc_vectors4_std_df)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"LcVGOJjU-wEX","colab_type":"code","colab":{}},"source":["test_labels, strengths = hdbscan.approximate_predict(clustering, doc_vectors4_std_df_umap)\n","test_labels"],"execution_count":null,"outputs":[]}]}